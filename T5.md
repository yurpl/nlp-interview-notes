- Pretraining: denoising scheme like BART
- Input: text with gaps
- Output: a series of phrases to fill those gaps
- **Original text:**
	- Thank you for inviting me to your party last week
- Input: Thank you for inviting me to your party last week
- Output: Thank you {X} to your party {Y} wee.
- Targets: {X} for inviting {Y} last {Z}
- Trained on colossal cleaned common crawl (C4): 750 GB of text
- Was one of the first papers to heavily focus on the scale and quality of pre-training data
- Another key focus: Multitasking between generation tasks 